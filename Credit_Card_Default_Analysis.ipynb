{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Thoughts on the Problem and the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high level, this dataset can be essentially split into two sections:\n",
    "\n",
    "1. The first section is the demographic and personal details of each credit card client with features: ID, limit balance in NT dollars, sex, education level, marriage status, and age - a total of 6 features.\n",
    "  \n",
    "2. The second section is the usage and payment history across 6 months, namely repayment status, monthly bill amount, and monthly payment amount, for each of the 6 months respectively. This accounts for 18 features.\n",
    "\n",
    "The 25th column, whether the client defaults the next month's payment (October), is what is known as \"y\" variable in machine learning terminology. We will be splitting this from the dataframe to create X_train and X_test respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset as pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "ID                                                                         \n",
       "1     20000.0    2          2         1   24      2      2     -1     -1   \n",
       "2    120000.0    2          2         2   26     -1      2      0      0   \n",
       "3     90000.0    2          2         2   34      0      0      0      0   \n",
       "4     50000.0    2          2         1   37      0      0      0      0   \n",
       "5     50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "    PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "ID         ...                                                                  \n",
       "1      -2  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "2       0  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "3       0  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "4       0  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "5       0  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "    PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "ID                                                            \n",
       "1        0.0       0.0       0.0                           1  \n",
       "2     1000.0       0.0    2000.0                           1  \n",
       "3     1000.0    1000.0    5000.0                           0  \n",
       "4     1100.0    1069.0    1000.0                           0  \n",
       "5     9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv file\n",
    "ccard_df = pd.read_csv(\"data/UCI_Credit_Card.csv\", index_col=0)\n",
    "ccard_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the data is split into train (70%) and test (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the columns to X and y\n",
    "X = ccard_df.drop(columns=\"default.payment.next.month\")\n",
    "y = ccard_df[\"default.payment.next.month\"]\n",
    "\n",
    "# Split the data into X and y, and train and test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=76)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21000 entries, 8959 to 2722\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   LIMIT_BAL                   21000 non-null  float64\n",
      " 1   SEX                         21000 non-null  int64  \n",
      " 2   EDUCATION                   21000 non-null  int64  \n",
      " 3   MARRIAGE                    21000 non-null  int64  \n",
      " 4   AGE                         21000 non-null  int64  \n",
      " 5   PAY_0                       21000 non-null  int64  \n",
      " 6   PAY_2                       21000 non-null  int64  \n",
      " 7   PAY_3                       21000 non-null  int64  \n",
      " 8   PAY_4                       21000 non-null  int64  \n",
      " 9   PAY_5                       21000 non-null  int64  \n",
      " 10  PAY_6                       21000 non-null  int64  \n",
      " 11  BILL_AMT1                   21000 non-null  float64\n",
      " 12  BILL_AMT2                   21000 non-null  float64\n",
      " 13  BILL_AMT3                   21000 non-null  float64\n",
      " 14  BILL_AMT4                   21000 non-null  float64\n",
      " 15  BILL_AMT5                   21000 non-null  float64\n",
      " 16  BILL_AMT6                   21000 non-null  float64\n",
      " 17  PAY_AMT1                    21000 non-null  float64\n",
      " 18  PAY_AMT2                    21000 non-null  float64\n",
      " 19  PAY_AMT3                    21000 non-null  float64\n",
      " 20  PAY_AMT4                    21000 non-null  float64\n",
      " 21  PAY_AMT5                    21000 non-null  float64\n",
      " 22  PAY_AMT6                    21000 non-null  float64\n",
      " 23  default.payment.next.month  21000 non-null  int64  \n",
      "dtypes: float64(13), int64(11)\n",
      "memory usage: 4.0 MB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The features were described thoroughly on Kaggle. This dataset seems appropriate and useful for a classification problem using machine learning. As visible from the above information, all the features have 21000 non-null values, indicating that there are no missing values, which indicates that the data set is complete and does not need imputation. We have only int64 and float64 objects, indicating that there are only numeric features, and no categorical features. This means we do not need to encode text data into numeric data for further analysis. However, the scales are different, as we have binary values (which starts at 1), ordinal encoded values, raw value of age, among others.\n",
    "\n",
    "### Some Potential Issues\n",
    "For the PAY_X features, the description in Kaggle is missing what 0 and -2 means. The description is \"(-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\". After looking through the discussion page, I found that \"-2 corresponds to there being no credit to pay; and 0 corresponds to a payment being made on time but not the total amount to clear the balance. (Another way to think about this is that the minimum payment is met.\" This is in line with the ordinal encoding order of -1 to 9, and therefore is fine as is.\n",
    "\n",
    "The feature \"MARRIAGE\" is ordinally encoded with values 0, 1, 2, 3. However, marriage status does not have an inherent order, and ordinal encoding is typically used in instances where there is an order or continuous relationship in the feature. Therefore, this would likely benefit from being one-hot encoded instead. The Kaggle does not have the definition for the 0, but the dataset has a few examples, where the marriage column has the value of 0. We are going to keep it and continue with the meaning that 0 means not willing to mention.\n",
    "\n",
    "The binary values (particularly, of the feature 'sex') use 1-indexing, meaning they are encoded as '1' and '2' instead of '0' and '1'. This can be an issue as models might wrongly interpret the values 1 and 2 to be ordinal or continuous data rather than categorical. This is fixed using the binary transformation with the help of One-Hot-Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>167563.508571</td>\n",
       "      <td>1.604381</td>\n",
       "      <td>1.843905</td>\n",
       "      <td>1.554667</td>\n",
       "      <td>35.412952</td>\n",
       "      <td>-0.012190</td>\n",
       "      <td>-0.132714</td>\n",
       "      <td>-0.168333</td>\n",
       "      <td>-0.223143</td>\n",
       "      <td>-0.265762</td>\n",
       "      <td>...</td>\n",
       "      <td>43039.813952</td>\n",
       "      <td>40121.889810</td>\n",
       "      <td>38623.497095</td>\n",
       "      <td>5601.265286</td>\n",
       "      <td>6.059441e+03</td>\n",
       "      <td>5204.302571</td>\n",
       "      <td>4889.281333</td>\n",
       "      <td>4782.900857</td>\n",
       "      <td>5162.918714</td>\n",
       "      <td>0.221857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>129919.112502</td>\n",
       "      <td>0.488995</td>\n",
       "      <td>0.789845</td>\n",
       "      <td>0.521970</td>\n",
       "      <td>9.136302</td>\n",
       "      <td>1.121864</td>\n",
       "      <td>1.196554</td>\n",
       "      <td>1.195375</td>\n",
       "      <td>1.165490</td>\n",
       "      <td>1.134210</td>\n",
       "      <td>...</td>\n",
       "      <td>63817.414980</td>\n",
       "      <td>60400.798292</td>\n",
       "      <td>59055.005208</td>\n",
       "      <td>16239.423781</td>\n",
       "      <td>2.407470e+04</td>\n",
       "      <td>16865.645456</td>\n",
       "      <td>16486.840852</td>\n",
       "      <td>15431.523094</td>\n",
       "      <td>17170.608569</td>\n",
       "      <td>0.415505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-170000.000000</td>\n",
       "      <td>-81334.000000</td>\n",
       "      <td>-209051.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1266.250000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>8.200000e+02</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>257.750000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>140000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18990.000000</td>\n",
       "      <td>18091.000000</td>\n",
       "      <td>17127.000000</td>\n",
       "      <td>2112.500000</td>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>1801.500000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>240000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54740.000000</td>\n",
       "      <td>50065.250000</td>\n",
       "      <td>48950.500000</td>\n",
       "      <td>5012.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>4531.250000</td>\n",
       "      <td>4048.500000</td>\n",
       "      <td>4078.000000</td>\n",
       "      <td>4001.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>800000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>706864.000000</td>\n",
       "      <td>823540.000000</td>\n",
       "      <td>568638.000000</td>\n",
       "      <td>873552.000000</td>\n",
       "      <td>1.684259e+06</td>\n",
       "      <td>889043.000000</td>\n",
       "      <td>621000.000000</td>\n",
       "      <td>426529.000000</td>\n",
       "      <td>528666.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           LIMIT_BAL           SEX     EDUCATION      MARRIAGE           AGE  \\\n",
       "count   21000.000000  21000.000000  21000.000000  21000.000000  21000.000000   \n",
       "mean   167563.508571      1.604381      1.843905      1.554667     35.412952   \n",
       "std    129919.112502      0.488995      0.789845      0.521970      9.136302   \n",
       "min     10000.000000      1.000000      0.000000      0.000000     21.000000   \n",
       "25%     50000.000000      1.000000      1.000000      1.000000     28.000000   \n",
       "50%    140000.000000      2.000000      2.000000      2.000000     34.000000   \n",
       "75%    240000.000000      2.000000      2.000000      2.000000     41.000000   \n",
       "max    800000.000000      2.000000      6.000000      3.000000     79.000000   \n",
       "\n",
       "              PAY_0         PAY_2         PAY_3         PAY_4         PAY_5  \\\n",
       "count  21000.000000  21000.000000  21000.000000  21000.000000  21000.000000   \n",
       "mean      -0.012190     -0.132714     -0.168333     -0.223143     -0.265762   \n",
       "std        1.121864      1.196554      1.195375      1.165490      1.134210   \n",
       "min       -2.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
       "25%       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        8.000000      8.000000      8.000000      8.000000      8.000000   \n",
       "\n",
       "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
       "count  ...   21000.000000   21000.000000   21000.000000   21000.000000   \n",
       "mean   ...   43039.813952   40121.889810   38623.497095    5601.265286   \n",
       "std    ...   63817.414980   60400.798292   59055.005208   16239.423781   \n",
       "min    ... -170000.000000  -81334.000000 -209051.000000       0.000000   \n",
       "25%    ...    2300.000000    1800.000000    1266.250000    1000.000000   \n",
       "50%    ...   18990.000000   18091.000000   17127.000000    2112.500000   \n",
       "75%    ...   54740.000000   50065.250000   48950.500000    5012.000000   \n",
       "max    ...  706864.000000  823540.000000  568638.000000  873552.000000   \n",
       "\n",
       "           PAY_AMT2       PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
       "count  2.100000e+04   21000.000000   21000.000000   21000.000000   \n",
       "mean   6.059441e+03    5204.302571    4889.281333    4782.900857   \n",
       "std    2.407470e+04   16865.645456   16486.840852   15431.523094   \n",
       "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
       "25%    8.200000e+02     390.000000     291.000000     257.750000   \n",
       "50%    2.009000e+03    1801.500000    1500.000000    1500.000000   \n",
       "75%    5.000000e+03    4531.250000    4048.500000    4078.000000   \n",
       "max    1.684259e+06  889043.000000  621000.000000  426529.000000   \n",
       "\n",
       "            PAY_AMT6  default.payment.next.month  \n",
       "count   21000.000000                21000.000000  \n",
       "mean     5162.918714                    0.221857  \n",
       "std     17170.608569                    0.415505  \n",
       "min         0.000000                    0.000000  \n",
       "25%       150.000000                    0.000000  \n",
       "50%      1500.000000                    0.000000  \n",
       "75%      4001.000000                    0.000000  \n",
       "max    528666.000000                    1.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the describe table above, two summary statistics which I find useful:\n",
    "\n",
    "1. 0.222 mean value of defaults on payment for the next month indicate that there is slight class imbalance. There are more data points with the negative class (non-default) than positive class(default). This also allows me to expect the results of predictions to hover around 20-25% defaults.\n",
    "2. All the features have 21000 non-null values. There are no missing values in this dataset, which means imputation is not needed in the preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this visualization, I am exploring the data by overlaying the 0 and 1 classes on three features which are more relevant than others. For example, sex and marriage status do not inform a data scientist of a client's financial ability as much as education and limit balance might. As this is an EDA, these are just to get a better idea before creating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2',\n",
       "       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default.payment.next.month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cols = ['LIMIT_BAL', 'EDUCATION', 'PAY_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for feat in visualize_cols:\n",
    "    ax = train_df.groupby(\"default.payment.next.month\")[feat].plot.hist(bins=20, alpha=0.4, legend=True)\n",
    "    plt.xlabel(feat)\n",
    "    plt.title(\"Histogram of \" + feat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on each of the histograms\n",
    "1. Histogram of Limit Balance: The shape of both classes are very similar, with class 0 (not-default) being the majority of cases. This tells me that the limit balance might not be a great predictor of defaulting, as there is a consistent proportion of people in each limit balance bin that defaults.\n",
    "   \n",
    "2. Histogram of Education: Similar case as limit balance histogram. There is a consistent proportion of people in each education bin that defaults.\n",
    "\n",
    "3. Histogram of Pay 6: This is the latest payment status before the month to predict default status. As seen above, there is a strong correlation between defaulting and previous months' failure to pay. There is a disproportionately high number of defaults for values of PAY_6 2 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appropriate Metric: Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the classification metrics to predict whether the client will default, an appropriate metric would be recall. This is because, when a client defaults their loan, it incurs large costs to the bank. This means that the cost of false negatives is high, where false negatives are predictions labelling a client as non-default, when they will indeed default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Feature Transformations\n",
    "\n",
    "| Feature | Transformation | Explanation\n",
    "| --- | ----------- | ----- |\n",
    "| LIMIT_BAL | scaling |  A numeric feature with no missing values. Good idea to apply scaling, as the range of values (10,000 to 800,000) is quite different compared to other numeric features.|\n",
    "| SEX | one-hot encoding | A binary feature that uses 1 and 2 instead of 0 and 1. Simple transformation to decrease all values by 1 to make it standard binary values of 0 and 1.\n",
    "| EDUCATION | None | An ordinal feature where the highest education has the lowest integer value. \n",
    "| MARRIAGE | one-hot encoding | Originally an ordinal feature that should be one-hot encoded instead. Marriage status does not have an inherent order, and ordinal encoding is typically used in instances where there is an order or continuous relationship in the feature.\n",
    "| AGE | scaling |  A numeric feature with no missing values. Good idea to apply scaling, as the range of values (21 to 79) is quite different compared to other numeric features.|\n",
    "| PAY_X | none  |  An ordinal feature from -1 to 9 that does not need any preprocessing.\n",
    "| BILL_AMTX | scaling | A numerical feature representing amount of bill statement in previous 1 to 6 months. Good idea to apply scaling, as the range of values is quite different compared to other numeric features. |\n",
    "| PAY_AMTX | scaling  | A numerical feature representing payment amount in previous 1 to 6 months. Good idea to apply scaling, as the range of values is quite different compared to other numeric features.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types\n",
    "\n",
    "numeric_features = [\"LIMIT_BAL\", \"AGE\", \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]\n",
    "categorical_features = [\"MARRIAGE\"] # includes one-hot encoded\n",
    "binary_features = [\"SEX\"]\n",
    "ordinal_features = [\"EDUCATION\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"] # Note that this is already ordinal, no need to preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "binary_transformer = OneHotEncoder(drop = \"if_binary\")\n",
    "# Ordinal features do not need to be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),  \n",
    "    (categorical_transformer, categorical_features),\n",
    "    (binary_transformer, binary_features),\n",
    "    (\"passthrough\", ordinal_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>...</th>\n",
       "      <th>MARRIAGE_2</th>\n",
       "      <th>MARRIAGE_3</th>\n",
       "      <th>SEX_2</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.327292</td>\n",
       "      <td>0.939905</td>\n",
       "      <td>0.433921</td>\n",
       "      <td>0.516430</td>\n",
       "      <td>0.390631</td>\n",
       "      <td>0.255174</td>\n",
       "      <td>1.920088</td>\n",
       "      <td>1.212636</td>\n",
       "      <td>0.886674</td>\n",
       "      <td>-0.044007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.249673</td>\n",
       "      <td>-0.154656</td>\n",
       "      <td>2.094514</td>\n",
       "      <td>1.813839</td>\n",
       "      <td>-0.625196</td>\n",
       "      <td>-0.657545</td>\n",
       "      <td>-0.637820</td>\n",
       "      <td>2.253487</td>\n",
       "      <td>-0.005990</td>\n",
       "      <td>-0.188312</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.674001</td>\n",
       "      <td>-1.030305</td>\n",
       "      <td>0.377072</td>\n",
       "      <td>0.441322</td>\n",
       "      <td>0.396381</td>\n",
       "      <td>0.507760</td>\n",
       "      <td>0.291611</td>\n",
       "      <td>0.330471</td>\n",
       "      <td>-0.172502</td>\n",
       "      <td>-0.135392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.674001</td>\n",
       "      <td>1.049361</td>\n",
       "      <td>0.386305</td>\n",
       "      <td>0.456687</td>\n",
       "      <td>0.491731</td>\n",
       "      <td>0.568122</td>\n",
       "      <td>0.679148</td>\n",
       "      <td>0.712040</td>\n",
       "      <td>-0.152181</td>\n",
       "      <td>-0.122640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.674001</td>\n",
       "      <td>0.502080</td>\n",
       "      <td>-0.657617</td>\n",
       "      <td>-0.660957</td>\n",
       "      <td>-0.211069</td>\n",
       "      <td>-0.647344</td>\n",
       "      <td>-0.654508</td>\n",
       "      <td>-0.491003</td>\n",
       "      <td>-0.219611</td>\n",
       "      <td>1.085587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL       AGE  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0   1.327292  0.939905   0.433921   0.516430   0.390631   0.255174   1.920088   \n",
       "1   0.249673 -0.154656   2.094514   1.813839  -0.625196  -0.657545  -0.637820   \n",
       "2  -0.674001 -1.030305   0.377072   0.441322   0.396381   0.507760   0.291611   \n",
       "3  -0.674001  1.049361   0.386305   0.456687   0.491731   0.568122   0.679148   \n",
       "4  -0.674001  0.502080  -0.657617  -0.660957  -0.211069  -0.647344  -0.654508   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  ...  MARRIAGE_2  MARRIAGE_3  SEX_2  \\\n",
       "0   1.212636  0.886674 -0.044007  ...         1.0         0.0    0.0   \n",
       "1   2.253487 -0.005990 -0.188312  ...         1.0         0.0    1.0   \n",
       "2   0.330471 -0.172502 -0.135392  ...         0.0         0.0    1.0   \n",
       "3   0.712040 -0.152181 -0.122640  ...         0.0         0.0    1.0   \n",
       "4  -0.491003 -0.219611  1.085587  ...         0.0         0.0    1.0   \n",
       "\n",
       "   EDUCATION  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6  \n",
       "0        1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1        2.0    0.0    0.0    0.0   -1.0   -1.0   -1.0  \n",
       "2        2.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3        2.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4        2.0   -1.0   -1.0   -1.0   -1.0   -1.0   -1.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df = pd.DataFrame(preprocessor.fit_transform(X_train), columns = numeric_features + list(preprocessor.named_transformers_[\"onehotencoder-1\"].get_feature_names_out(categorical_features)) + list(preprocessor.named_transformers_[\"onehotencoder-2\"].get_feature_names_out(binary_features)) + ordinal_features)\n",
    "transformed_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Transformed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>...</th>\n",
       "      <th>MARRIAGE_2</th>\n",
       "      <th>MARRIAGE_3</th>\n",
       "      <th>SEX_2</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.639159e-17</td>\n",
       "      <td>-2.510584e-16</td>\n",
       "      <td>1.251909e-17</td>\n",
       "      <td>2.909842e-17</td>\n",
       "      <td>-6.022696e-17</td>\n",
       "      <td>9.812257e-18</td>\n",
       "      <td>-4.364763e-17</td>\n",
       "      <td>-1.725604e-17</td>\n",
       "      <td>4.906128e-18</td>\n",
       "      <td>1.353415e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534524</td>\n",
       "      <td>0.010952</td>\n",
       "      <td>0.604381</td>\n",
       "      <td>1.843905</td>\n",
       "      <td>-0.012190</td>\n",
       "      <td>-0.132714</td>\n",
       "      <td>-0.168333</td>\n",
       "      <td>-0.223143</td>\n",
       "      <td>-0.265762</td>\n",
       "      <td>-0.287381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>1.000024e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498819</td>\n",
       "      <td>0.104081</td>\n",
       "      <td>0.488995</td>\n",
       "      <td>0.789845</td>\n",
       "      <td>1.121864</td>\n",
       "      <td>1.196554</td>\n",
       "      <td>1.195375</td>\n",
       "      <td>1.165490</td>\n",
       "      <td>1.134210</td>\n",
       "      <td>1.152388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.212810e+00</td>\n",
       "      <td>-1.577585e+00</td>\n",
       "      <td>-2.942023e+00</td>\n",
       "      <td>-1.672338e+00</td>\n",
       "      <td>-2.941144e+00</td>\n",
       "      <td>-3.338350e+00</td>\n",
       "      <td>-2.010880e+00</td>\n",
       "      <td>-4.194063e+00</td>\n",
       "      <td>-3.449259e-01</td>\n",
       "      <td>-2.516994e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.049193e-01</td>\n",
       "      <td>-8.113927e-01</td>\n",
       "      <td>-6.459431e-01</td>\n",
       "      <td>-6.481262e-01</td>\n",
       "      <td>-6.370447e-01</td>\n",
       "      <td>-6.383960e-01</td>\n",
       "      <td>-6.344751e-01</td>\n",
       "      <td>-6.325990e-01</td>\n",
       "      <td>-2.833459e-01</td>\n",
       "      <td>-2.176379e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.121640e-01</td>\n",
       "      <td>-1.546562e-01</td>\n",
       "      <td>-3.950579e-01</td>\n",
       "      <td>-3.972948e-01</td>\n",
       "      <td>-3.868705e-01</td>\n",
       "      <td>-3.768624e-01</td>\n",
       "      <td>-3.647537e-01</td>\n",
       "      <td>-3.640167e-01</td>\n",
       "      <td>-2.148382e-01</td>\n",
       "      <td>-1.682488e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.575640e-01</td>\n",
       "      <td>6.115363e-01</td>\n",
       "      <td>2.175711e-01</td>\n",
       "      <td>2.086617e-01</td>\n",
       "      <td>1.937376e-01</td>\n",
       "      <td>1.833428e-01</td>\n",
       "      <td>1.646269e-01</td>\n",
       "      <td>1.748751e-01</td>\n",
       "      <td>-3.628696e-02</td>\n",
       "      <td>-4.400748e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.868041e+00</td>\n",
       "      <td>4.770867e+00</td>\n",
       "      <td>9.446184e+00</td>\n",
       "      <td>9.788265e+00</td>\n",
       "      <td>2.330441e+01</td>\n",
       "      <td>1.040218e+01</td>\n",
       "      <td>1.297064e+01</td>\n",
       "      <td>8.975143e+00</td>\n",
       "      <td>5.344841e+01</td>\n",
       "      <td>6.970969e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LIMIT_BAL           AGE     BILL_AMT1     BILL_AMT2     BILL_AMT3  \\\n",
       "count  2.100000e+04  2.100000e+04  2.100000e+04  2.100000e+04  2.100000e+04   \n",
       "mean   2.639159e-17 -2.510584e-16  1.251909e-17  2.909842e-17 -6.022696e-17   \n",
       "std    1.000024e+00  1.000024e+00  1.000024e+00  1.000024e+00  1.000024e+00   \n",
       "min   -1.212810e+00 -1.577585e+00 -2.942023e+00 -1.672338e+00 -2.941144e+00   \n",
       "25%   -9.049193e-01 -8.113927e-01 -6.459431e-01 -6.481262e-01 -6.370447e-01   \n",
       "50%   -2.121640e-01 -1.546562e-01 -3.950579e-01 -3.972948e-01 -3.868705e-01   \n",
       "75%    5.575640e-01  6.115363e-01  2.175711e-01  2.086617e-01  1.937376e-01   \n",
       "max    4.868041e+00  4.770867e+00  9.446184e+00  9.788265e+00  2.330441e+01   \n",
       "\n",
       "          BILL_AMT4     BILL_AMT5     BILL_AMT6      PAY_AMT1      PAY_AMT2  \\\n",
       "count  2.100000e+04  2.100000e+04  2.100000e+04  2.100000e+04  2.100000e+04   \n",
       "mean   9.812257e-18 -4.364763e-17 -1.725604e-17  4.906128e-18  1.353415e-17   \n",
       "std    1.000024e+00  1.000024e+00  1.000024e+00  1.000024e+00  1.000024e+00   \n",
       "min   -3.338350e+00 -2.010880e+00 -4.194063e+00 -3.449259e-01 -2.516994e-01   \n",
       "25%   -6.383960e-01 -6.344751e-01 -6.325990e-01 -2.833459e-01 -2.176379e-01   \n",
       "50%   -3.768624e-01 -3.647537e-01 -3.640167e-01 -2.148382e-01 -1.682488e-01   \n",
       "75%    1.833428e-01  1.646269e-01  1.748751e-01 -3.628696e-02 -4.400748e-02   \n",
       "max    1.040218e+01  1.297064e+01  8.975143e+00  5.344841e+01  6.970969e+01   \n",
       "\n",
       "       ...    MARRIAGE_2    MARRIAGE_3         SEX_2     EDUCATION  \\\n",
       "count  ...  21000.000000  21000.000000  21000.000000  21000.000000   \n",
       "mean   ...      0.534524      0.010952      0.604381      1.843905   \n",
       "std    ...      0.498819      0.104081      0.488995      0.789845   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      1.000000   \n",
       "50%    ...      1.000000      0.000000      1.000000      2.000000   \n",
       "75%    ...      1.000000      0.000000      1.000000      2.000000   \n",
       "max    ...      1.000000      1.000000      1.000000      6.000000   \n",
       "\n",
       "              PAY_0         PAY_2         PAY_3         PAY_4         PAY_5  \\\n",
       "count  21000.000000  21000.000000  21000.000000  21000.000000  21000.000000   \n",
       "mean      -0.012190     -0.132714     -0.168333     -0.223143     -0.265762   \n",
       "std        1.121864      1.196554      1.195375      1.165490      1.134210   \n",
       "min       -2.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
       "25%       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        8.000000      8.000000      8.000000      8.000000      8.000000   \n",
       "\n",
       "              PAY_6  \n",
       "count  21000.000000  \n",
       "mean      -0.287381  \n",
       "std        1.152388  \n",
       "min       -2.000000  \n",
       "25%       -1.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        8.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = numeric_features + list(preprocessor.named_transformers_[\"onehotencoder-1\"].get_feature_names_out(categorical_features)) + list(preprocessor.named_transformers_[\"onehotencoder-2\"].get_feature_names_out(binary_features)) + ordinal_features \n",
    "\n",
    "transformed_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(column_names) == set(transformed_df.columns.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the linear models that we have learned in this course. One of the linear models that is going to be used is logistic regression. One of the reasons behind choosing the logistic regression is that the model is able to use the linear function and compute the raw score, which later is used in the sigmoid function to predict, if the example, is a positive (1) or negative (0). Thus, it maintains its simplicity of running the model on a linear function, which is easy to understand and interpret. Now, lets create the logistic regression model with the default hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs): #Adapted from lecture notes\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic Regression Default</th>\n",
       "      <td>0.033 (+/- 0.002)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.810 (+/- 0.005)</td>\n",
       "      <td>0.810 (+/- 0.001)</td>\n",
       "      <td>0.238 (+/- 0.017)</td>\n",
       "      <td>0.239 (+/- 0.005)</td>\n",
       "      <td>0.718 (+/- 0.025)</td>\n",
       "      <td>0.718 (+/- 0.006)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      fit_time         score_time  \\\n",
       "logistic Regression Default  0.033 (+/- 0.002)  0.003 (+/- 0.000)   \n",
       "\n",
       "                                 test_accuracy     train_accuracy  \\\n",
       "logistic Regression Default  0.810 (+/- 0.005)  0.810 (+/- 0.001)   \n",
       "\n",
       "                                   test_recall       train_recall  \\\n",
       "logistic Regression Default  0.238 (+/- 0.017)  0.239 (+/- 0.005)   \n",
       "\n",
       "                                test_precision    train_precision  \n",
       "logistic Regression Default  0.718 (+/- 0.025)  0.718 (+/- 0.006)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "default_result = {}\n",
    "lr = LogisticRegression()\n",
    "scoring = [\"accuracy\", \"recall\", \"precision\"]\n",
    "scores = mean_std_cross_val_scores(lr, transformed_df, y_train, cv = 5, return_train_score = True, scoring = scoring)\n",
    "default_result[\"logistic Regression Default\"] = scores\n",
    "display(pd.DataFrame(default_result).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous code outputs, it can be concluded that there is less difference between the train and test score, with the train score being a little smaller score, which means that the logistic regression model, with the default hyperparameter of C = 1, may lead to under-fitting. Thus, some hyperparameter optimization may be needed. Now, let's move forward towards the computation of the hyperparameter optimization. One important thing to note is that we have very little recall, which is indeed a problem, as we want our recall to be of a reasonable number (at least 75% in our case), this is because we have a class imbalance, which can be resolved by adjusting the class weight of each class. Following are the different values of the hyperparameter C and also the class_weight that will be used in hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "              \"class_weight\":[\"balanced\", {0:1, 1:3}, {0:1, 1:5}, {0:1, 1:7}, {0:1, 1:9}]}\n",
    "result_dict = {\"C\":[], \"class_weight\":[], \"train_score\":[], \"test_score\":[], \"train_recall\":[], \"test_recall\":[], \"train_precision\":[], \"test_precision\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for c in param_grid[\"C\"]:\n",
    "    for weight in param_grid[\"class_weight\"]:\n",
    "        lr = LogisticRegression(C = c, class_weight = weight)\n",
    "        scores = mean_std_cross_val_scores(lr, transformed_df, y_train, cv = 5, return_train_score = True, scoring = scoring)\n",
    "        results[\"LR C = \" + str(c) + \" weight = \" + str(weight)] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting results from the hyperparameter optimzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR C = 0.001 weight = balanced</th>\n",
       "      <td>0.012 (+/- 0.002)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.690 (+/- 0.006)</td>\n",
       "      <td>0.692 (+/- 0.004)</td>\n",
       "      <td>0.625 (+/- 0.013)</td>\n",
       "      <td>0.628 (+/- 0.002)</td>\n",
       "      <td>0.380 (+/- 0.006)</td>\n",
       "      <td>0.382 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.001 weight = {0: 1, 1: 3}</th>\n",
       "      <td>0.013 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.774 (+/- 0.007)</td>\n",
       "      <td>0.775 (+/- 0.003)</td>\n",
       "      <td>0.519 (+/- 0.009)</td>\n",
       "      <td>0.522 (+/- 0.004)</td>\n",
       "      <td>0.491 (+/- 0.015)</td>\n",
       "      <td>0.493 (+/- 0.006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.001 weight = {0: 1, 1: 5}</th>\n",
       "      <td>0.015 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.495 (+/- 0.005)</td>\n",
       "      <td>0.496 (+/- 0.003)</td>\n",
       "      <td>0.789 (+/- 0.010)</td>\n",
       "      <td>0.790 (+/- 0.003)</td>\n",
       "      <td>0.277 (+/- 0.003)</td>\n",
       "      <td>0.277 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.001 weight = {0: 1, 1: 7}</th>\n",
       "      <td>0.018 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.375 (+/- 0.007)</td>\n",
       "      <td>0.374 (+/- 0.004)</td>\n",
       "      <td>0.898 (+/- 0.013)</td>\n",
       "      <td>0.896 (+/- 0.001)</td>\n",
       "      <td>0.248 (+/- 0.003)</td>\n",
       "      <td>0.248 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.001 weight = {0: 1, 1: 9}</th>\n",
       "      <td>0.017 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.317 (+/- 0.005)</td>\n",
       "      <td>0.319 (+/- 0.002)</td>\n",
       "      <td>0.932 (+/- 0.009)</td>\n",
       "      <td>0.932 (+/- 0.002)</td>\n",
       "      <td>0.237 (+/- 0.002)</td>\n",
       "      <td>0.237 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.01 weight = balanced</th>\n",
       "      <td>0.017 (+/- 0.002)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.691 (+/- 0.007)</td>\n",
       "      <td>0.692 (+/- 0.004)</td>\n",
       "      <td>0.634 (+/- 0.012)</td>\n",
       "      <td>0.637 (+/- 0.003)</td>\n",
       "      <td>0.382 (+/- 0.006)</td>\n",
       "      <td>0.383 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.01 weight = {0: 1, 1: 3}</th>\n",
       "      <td>0.019 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.762 (+/- 0.009)</td>\n",
       "      <td>0.762 (+/- 0.004)</td>\n",
       "      <td>0.547 (+/- 0.008)</td>\n",
       "      <td>0.547 (+/- 0.005)</td>\n",
       "      <td>0.470 (+/- 0.017)</td>\n",
       "      <td>0.469 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.01 weight = {0: 1, 1: 5}</th>\n",
       "      <td>0.025 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.508 (+/- 0.006)</td>\n",
       "      <td>0.509 (+/- 0.004)</td>\n",
       "      <td>0.778 (+/- 0.011)</td>\n",
       "      <td>0.783 (+/- 0.003)</td>\n",
       "      <td>0.281 (+/- 0.003)</td>\n",
       "      <td>0.282 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.01 weight = {0: 1, 1: 7}</th>\n",
       "      <td>0.031 (+/- 0.002)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.389 (+/- 0.007)</td>\n",
       "      <td>0.389 (+/- 0.004)</td>\n",
       "      <td>0.887 (+/- 0.014)</td>\n",
       "      <td>0.886 (+/- 0.001)</td>\n",
       "      <td>0.251 (+/- 0.003)</td>\n",
       "      <td>0.251 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.01 weight = {0: 1, 1: 9}</th>\n",
       "      <td>0.032 (+/- 0.002)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.327 (+/- 0.005)</td>\n",
       "      <td>0.327 (+/- 0.003)</td>\n",
       "      <td>0.926 (+/- 0.011)</td>\n",
       "      <td>0.929 (+/- 0.002)</td>\n",
       "      <td>0.238 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.1 weight = balanced</th>\n",
       "      <td>0.031 (+/- 0.002)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.690 (+/- 0.008)</td>\n",
       "      <td>0.691 (+/- 0.005)</td>\n",
       "      <td>0.635 (+/- 0.011)</td>\n",
       "      <td>0.636 (+/- 0.004)</td>\n",
       "      <td>0.381 (+/- 0.007)</td>\n",
       "      <td>0.382 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.1 weight = {0: 1, 1: 3}</th>\n",
       "      <td>0.033 (+/- 0.002)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.759 (+/- 0.010)</td>\n",
       "      <td>0.760 (+/- 0.004)</td>\n",
       "      <td>0.549 (+/- 0.010)</td>\n",
       "      <td>0.551 (+/- 0.005)</td>\n",
       "      <td>0.464 (+/- 0.018)</td>\n",
       "      <td>0.466 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.1 weight = {0: 1, 1: 5}</th>\n",
       "      <td>0.034 (+/- 0.003)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.509 (+/- 0.006)</td>\n",
       "      <td>0.511 (+/- 0.004)</td>\n",
       "      <td>0.777 (+/- 0.010)</td>\n",
       "      <td>0.781 (+/- 0.003)</td>\n",
       "      <td>0.281 (+/- 0.003)</td>\n",
       "      <td>0.282 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.1 weight = {0: 1, 1: 7}</th>\n",
       "      <td>0.039 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.391 (+/- 0.008)</td>\n",
       "      <td>0.391 (+/- 0.003)</td>\n",
       "      <td>0.885 (+/- 0.014)</td>\n",
       "      <td>0.885 (+/- 0.002)</td>\n",
       "      <td>0.252 (+/- 0.003)</td>\n",
       "      <td>0.252 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 0.1 weight = {0: 1, 1: 9}</th>\n",
       "      <td>0.043 (+/- 0.004)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.329 (+/- 0.006)</td>\n",
       "      <td>0.329 (+/- 0.003)</td>\n",
       "      <td>0.925 (+/- 0.011)</td>\n",
       "      <td>0.928 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 1 weight = balanced</th>\n",
       "      <td>0.037 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.690 (+/- 0.008)</td>\n",
       "      <td>0.691 (+/- 0.005)</td>\n",
       "      <td>0.635 (+/- 0.011)</td>\n",
       "      <td>0.636 (+/- 0.004)</td>\n",
       "      <td>0.381 (+/- 0.007)</td>\n",
       "      <td>0.382 (+/- 0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 1 weight = {0: 1, 1: 3}</th>\n",
       "      <td>0.034 (+/- 0.002)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.759 (+/- 0.010)</td>\n",
       "      <td>0.760 (+/- 0.004)</td>\n",
       "      <td>0.550 (+/- 0.009)</td>\n",
       "      <td>0.552 (+/- 0.005)</td>\n",
       "      <td>0.464 (+/- 0.018)</td>\n",
       "      <td>0.466 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 1 weight = {0: 1, 1: 5}</th>\n",
       "      <td>0.038 (+/- 0.003)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.509 (+/- 0.007)</td>\n",
       "      <td>0.511 (+/- 0.004)</td>\n",
       "      <td>0.777 (+/- 0.010)</td>\n",
       "      <td>0.781 (+/- 0.003)</td>\n",
       "      <td>0.281 (+/- 0.003)</td>\n",
       "      <td>0.282 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 1 weight = {0: 1, 1: 7}</th>\n",
       "      <td>0.045 (+/- 0.003)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.391 (+/- 0.008)</td>\n",
       "      <td>0.391 (+/- 0.003)</td>\n",
       "      <td>0.884 (+/- 0.014)</td>\n",
       "      <td>0.885 (+/- 0.002)</td>\n",
       "      <td>0.252 (+/- 0.003)</td>\n",
       "      <td>0.252 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 1 weight = {0: 1, 1: 9}</th>\n",
       "      <td>0.046 (+/- 0.005)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.329 (+/- 0.006)</td>\n",
       "      <td>0.329 (+/- 0.003)</td>\n",
       "      <td>0.924 (+/- 0.011)</td>\n",
       "      <td>0.928 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 10 weight = balanced</th>\n",
       "      <td>0.038 (+/- 0.003)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.689 (+/- 0.007)</td>\n",
       "      <td>0.691 (+/- 0.005)</td>\n",
       "      <td>0.635 (+/- 0.011)</td>\n",
       "      <td>0.637 (+/- 0.004)</td>\n",
       "      <td>0.380 (+/- 0.006)</td>\n",
       "      <td>0.382 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 10 weight = {0: 1, 1: 3}</th>\n",
       "      <td>0.036 (+/- 0.003)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.758 (+/- 0.010)</td>\n",
       "      <td>0.760 (+/- 0.004)</td>\n",
       "      <td>0.551 (+/- 0.009)</td>\n",
       "      <td>0.552 (+/- 0.006)</td>\n",
       "      <td>0.463 (+/- 0.018)</td>\n",
       "      <td>0.466 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 10 weight = {0: 1, 1: 5}</th>\n",
       "      <td>0.039 (+/- 0.003)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.510 (+/- 0.007)</td>\n",
       "      <td>0.511 (+/- 0.004)</td>\n",
       "      <td>0.777 (+/- 0.009)</td>\n",
       "      <td>0.781 (+/- 0.003)</td>\n",
       "      <td>0.281 (+/- 0.003)</td>\n",
       "      <td>0.282 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 10 weight = {0: 1, 1: 7}</th>\n",
       "      <td>0.047 (+/- 0.003)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.392 (+/- 0.008)</td>\n",
       "      <td>0.391 (+/- 0.003)</td>\n",
       "      <td>0.884 (+/- 0.014)</td>\n",
       "      <td>0.885 (+/- 0.002)</td>\n",
       "      <td>0.252 (+/- 0.003)</td>\n",
       "      <td>0.252 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 10 weight = {0: 1, 1: 9}</th>\n",
       "      <td>0.046 (+/- 0.006)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.329 (+/- 0.006)</td>\n",
       "      <td>0.330 (+/- 0.003)</td>\n",
       "      <td>0.925 (+/- 0.011)</td>\n",
       "      <td>0.928 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 100 weight = balanced</th>\n",
       "      <td>0.038 (+/- 0.003)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.689 (+/- 0.007)</td>\n",
       "      <td>0.691 (+/- 0.005)</td>\n",
       "      <td>0.635 (+/- 0.011)</td>\n",
       "      <td>0.637 (+/- 0.004)</td>\n",
       "      <td>0.381 (+/- 0.007)</td>\n",
       "      <td>0.382 (+/- 0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 100 weight = {0: 1, 1: 3}</th>\n",
       "      <td>0.037 (+/- 0.005)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.758 (+/- 0.010)</td>\n",
       "      <td>0.760 (+/- 0.004)</td>\n",
       "      <td>0.551 (+/- 0.009)</td>\n",
       "      <td>0.552 (+/- 0.006)</td>\n",
       "      <td>0.463 (+/- 0.018)</td>\n",
       "      <td>0.466 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 100 weight = {0: 1, 1: 5}</th>\n",
       "      <td>0.038 (+/- 0.004)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.510 (+/- 0.007)</td>\n",
       "      <td>0.511 (+/- 0.004)</td>\n",
       "      <td>0.777 (+/- 0.010)</td>\n",
       "      <td>0.781 (+/- 0.003)</td>\n",
       "      <td>0.281 (+/- 0.003)</td>\n",
       "      <td>0.282 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 100 weight = {0: 1, 1: 7}</th>\n",
       "      <td>0.047 (+/- 0.005)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.392 (+/- 0.008)</td>\n",
       "      <td>0.391 (+/- 0.003)</td>\n",
       "      <td>0.885 (+/- 0.014)</td>\n",
       "      <td>0.885 (+/- 0.002)</td>\n",
       "      <td>0.252 (+/- 0.003)</td>\n",
       "      <td>0.252 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR C = 100 weight = {0: 1, 1: 9}</th>\n",
       "      <td>0.046 (+/- 0.006)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.329 (+/- 0.006)</td>\n",
       "      <td>0.330 (+/- 0.003)</td>\n",
       "      <td>0.925 (+/- 0.011)</td>\n",
       "      <td>0.928 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.002)</td>\n",
       "      <td>0.239 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             fit_time         score_time  \\\n",
       "LR C = 0.001 weight = balanced      0.012 (+/- 0.002)  0.003 (+/- 0.000)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 3}  0.013 (+/- 0.001)  0.003 (+/- 0.000)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 5}  0.015 (+/- 0.001)  0.003 (+/- 0.000)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 7}  0.018 (+/- 0.001)  0.003 (+/- 0.000)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 9}  0.017 (+/- 0.001)  0.003 (+/- 0.000)   \n",
       "LR C = 0.01 weight = balanced       0.017 (+/- 0.002)  0.003 (+/- 0.000)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 3}   0.019 (+/- 0.001)  0.003 (+/- 0.000)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 5}   0.025 (+/- 0.001)  0.003 (+/- 0.000)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 7}   0.031 (+/- 0.002)  0.003 (+/- 0.000)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 9}   0.032 (+/- 0.002)  0.003 (+/- 0.000)   \n",
       "LR C = 0.1 weight = balanced        0.031 (+/- 0.002)  0.003 (+/- 0.000)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 3}    0.033 (+/- 0.002)  0.003 (+/- 0.000)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 5}    0.034 (+/- 0.003)  0.003 (+/- 0.000)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 7}    0.039 (+/- 0.001)  0.003 (+/- 0.000)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 9}    0.043 (+/- 0.004)  0.003 (+/- 0.000)   \n",
       "LR C = 1 weight = balanced          0.037 (+/- 0.001)  0.003 (+/- 0.000)   \n",
       "LR C = 1 weight = {0: 1, 1: 3}      0.034 (+/- 0.002)  0.003 (+/- 0.000)   \n",
       "LR C = 1 weight = {0: 1, 1: 5}      0.038 (+/- 0.003)  0.003 (+/- 0.000)   \n",
       "LR C = 1 weight = {0: 1, 1: 7}      0.045 (+/- 0.003)  0.003 (+/- 0.000)   \n",
       "LR C = 1 weight = {0: 1, 1: 9}      0.046 (+/- 0.005)  0.003 (+/- 0.000)   \n",
       "LR C = 10 weight = balanced         0.038 (+/- 0.003)  0.003 (+/- 0.000)   \n",
       "LR C = 10 weight = {0: 1, 1: 3}     0.036 (+/- 0.003)  0.003 (+/- 0.000)   \n",
       "LR C = 10 weight = {0: 1, 1: 5}     0.039 (+/- 0.003)  0.003 (+/- 0.000)   \n",
       "LR C = 10 weight = {0: 1, 1: 7}     0.047 (+/- 0.003)  0.003 (+/- 0.000)   \n",
       "LR C = 10 weight = {0: 1, 1: 9}     0.046 (+/- 0.006)  0.003 (+/- 0.000)   \n",
       "LR C = 100 weight = balanced        0.038 (+/- 0.003)  0.003 (+/- 0.000)   \n",
       "LR C = 100 weight = {0: 1, 1: 3}    0.037 (+/- 0.005)  0.003 (+/- 0.000)   \n",
       "LR C = 100 weight = {0: 1, 1: 5}    0.038 (+/- 0.004)  0.003 (+/- 0.000)   \n",
       "LR C = 100 weight = {0: 1, 1: 7}    0.047 (+/- 0.005)  0.003 (+/- 0.000)   \n",
       "LR C = 100 weight = {0: 1, 1: 9}    0.046 (+/- 0.006)  0.003 (+/- 0.000)   \n",
       "\n",
       "                                        test_accuracy     train_accuracy  \\\n",
       "LR C = 0.001 weight = balanced      0.690 (+/- 0.006)  0.692 (+/- 0.004)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 3}  0.774 (+/- 0.007)  0.775 (+/- 0.003)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 5}  0.495 (+/- 0.005)  0.496 (+/- 0.003)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 7}  0.375 (+/- 0.007)  0.374 (+/- 0.004)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 9}  0.317 (+/- 0.005)  0.319 (+/- 0.002)   \n",
       "LR C = 0.01 weight = balanced       0.691 (+/- 0.007)  0.692 (+/- 0.004)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 3}   0.762 (+/- 0.009)  0.762 (+/- 0.004)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 5}   0.508 (+/- 0.006)  0.509 (+/- 0.004)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 7}   0.389 (+/- 0.007)  0.389 (+/- 0.004)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 9}   0.327 (+/- 0.005)  0.327 (+/- 0.003)   \n",
       "LR C = 0.1 weight = balanced        0.690 (+/- 0.008)  0.691 (+/- 0.005)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 3}    0.759 (+/- 0.010)  0.760 (+/- 0.004)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 5}    0.509 (+/- 0.006)  0.511 (+/- 0.004)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 7}    0.391 (+/- 0.008)  0.391 (+/- 0.003)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 9}    0.329 (+/- 0.006)  0.329 (+/- 0.003)   \n",
       "LR C = 1 weight = balanced          0.690 (+/- 0.008)  0.691 (+/- 0.005)   \n",
       "LR C = 1 weight = {0: 1, 1: 3}      0.759 (+/- 0.010)  0.760 (+/- 0.004)   \n",
       "LR C = 1 weight = {0: 1, 1: 5}      0.509 (+/- 0.007)  0.511 (+/- 0.004)   \n",
       "LR C = 1 weight = {0: 1, 1: 7}      0.391 (+/- 0.008)  0.391 (+/- 0.003)   \n",
       "LR C = 1 weight = {0: 1, 1: 9}      0.329 (+/- 0.006)  0.329 (+/- 0.003)   \n",
       "LR C = 10 weight = balanced         0.689 (+/- 0.007)  0.691 (+/- 0.005)   \n",
       "LR C = 10 weight = {0: 1, 1: 3}     0.758 (+/- 0.010)  0.760 (+/- 0.004)   \n",
       "LR C = 10 weight = {0: 1, 1: 5}     0.510 (+/- 0.007)  0.511 (+/- 0.004)   \n",
       "LR C = 10 weight = {0: 1, 1: 7}     0.392 (+/- 0.008)  0.391 (+/- 0.003)   \n",
       "LR C = 10 weight = {0: 1, 1: 9}     0.329 (+/- 0.006)  0.330 (+/- 0.003)   \n",
       "LR C = 100 weight = balanced        0.689 (+/- 0.007)  0.691 (+/- 0.005)   \n",
       "LR C = 100 weight = {0: 1, 1: 3}    0.758 (+/- 0.010)  0.760 (+/- 0.004)   \n",
       "LR C = 100 weight = {0: 1, 1: 5}    0.510 (+/- 0.007)  0.511 (+/- 0.004)   \n",
       "LR C = 100 weight = {0: 1, 1: 7}    0.392 (+/- 0.008)  0.391 (+/- 0.003)   \n",
       "LR C = 100 weight = {0: 1, 1: 9}    0.329 (+/- 0.006)  0.330 (+/- 0.003)   \n",
       "\n",
       "                                          test_recall       train_recall  \\\n",
       "LR C = 0.001 weight = balanced      0.625 (+/- 0.013)  0.628 (+/- 0.002)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 3}  0.519 (+/- 0.009)  0.522 (+/- 0.004)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 5}  0.789 (+/- 0.010)  0.790 (+/- 0.003)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 7}  0.898 (+/- 0.013)  0.896 (+/- 0.001)   \n",
       "LR C = 0.001 weight = {0: 1, 1: 9}  0.932 (+/- 0.009)  0.932 (+/- 0.002)   \n",
       "LR C = 0.01 weight = balanced       0.634 (+/- 0.012)  0.637 (+/- 0.003)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 3}   0.547 (+/- 0.008)  0.547 (+/- 0.005)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 5}   0.778 (+/- 0.011)  0.783 (+/- 0.003)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 7}   0.887 (+/- 0.014)  0.886 (+/- 0.001)   \n",
       "LR C = 0.01 weight = {0: 1, 1: 9}   0.926 (+/- 0.011)  0.929 (+/- 0.002)   \n",
       "LR C = 0.1 weight = balanced        0.635 (+/- 0.011)  0.636 (+/- 0.004)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 3}    0.549 (+/- 0.010)  0.551 (+/- 0.005)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 5}    0.777 (+/- 0.010)  0.781 (+/- 0.003)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 7}    0.885 (+/- 0.014)  0.885 (+/- 0.002)   \n",
       "LR C = 0.1 weight = {0: 1, 1: 9}    0.925 (+/- 0.011)  0.928 (+/- 0.002)   \n",
       "LR C = 1 weight = balanced          0.635 (+/- 0.011)  0.636 (+/- 0.004)   \n",
       "LR C = 1 weight = {0: 1, 1: 3}      0.550 (+/- 0.009)  0.552 (+/- 0.005)   \n",
       "LR C = 1 weight = {0: 1, 1: 5}      0.777 (+/- 0.010)  0.781 (+/- 0.003)   \n",
       "LR C = 1 weight = {0: 1, 1: 7}      0.884 (+/- 0.014)  0.885 (+/- 0.002)   \n",
       "LR C = 1 weight = {0: 1, 1: 9}      0.924 (+/- 0.011)  0.928 (+/- 0.002)   \n",
       "LR C = 10 weight = balanced         0.635 (+/- 0.011)  0.637 (+/- 0.004)   \n",
       "LR C = 10 weight = {0: 1, 1: 3}     0.551 (+/- 0.009)  0.552 (+/- 0.006)   \n",
       "LR C = 10 weight = {0: 1, 1: 5}     0.777 (+/- 0.009)  0.781 (+/- 0.003)   \n",
       "LR C = 10 weight = {0: 1, 1: 7}     0.884 (+/- 0.014)  0.885 (+/- 0.002)   \n",
       "LR C = 10 weight = {0: 1, 1: 9}     0.925 (+/- 0.011)  0.928 (+/- 0.002)   \n",
       "LR C = 100 weight = balanced        0.635 (+/- 0.011)  0.637 (+/- 0.004)   \n",
       "LR C = 100 weight = {0: 1, 1: 3}    0.551 (+/- 0.009)  0.552 (+/- 0.006)   \n",
       "LR C = 100 weight = {0: 1, 1: 5}    0.777 (+/- 0.010)  0.781 (+/- 0.003)   \n",
       "LR C = 100 weight = {0: 1, 1: 7}    0.885 (+/- 0.014)  0.885 (+/- 0.002)   \n",
       "LR C = 100 weight = {0: 1, 1: 9}    0.925 (+/- 0.011)  0.928 (+/- 0.002)   \n",
       "\n",
       "                                       test_precision    train_precision  \n",
       "LR C = 0.001 weight = balanced      0.380 (+/- 0.006)  0.382 (+/- 0.004)  \n",
       "LR C = 0.001 weight = {0: 1, 1: 3}  0.491 (+/- 0.015)  0.493 (+/- 0.006)  \n",
       "LR C = 0.001 weight = {0: 1, 1: 5}  0.277 (+/- 0.003)  0.277 (+/- 0.001)  \n",
       "LR C = 0.001 weight = {0: 1, 1: 7}  0.248 (+/- 0.003)  0.248 (+/- 0.001)  \n",
       "LR C = 0.001 weight = {0: 1, 1: 9}  0.237 (+/- 0.002)  0.237 (+/- 0.001)  \n",
       "LR C = 0.01 weight = balanced       0.382 (+/- 0.006)  0.383 (+/- 0.004)  \n",
       "LR C = 0.01 weight = {0: 1, 1: 3}   0.470 (+/- 0.017)  0.469 (+/- 0.007)  \n",
       "LR C = 0.01 weight = {0: 1, 1: 5}   0.281 (+/- 0.003)  0.282 (+/- 0.002)  \n",
       "LR C = 0.01 weight = {0: 1, 1: 7}   0.251 (+/- 0.003)  0.251 (+/- 0.001)  \n",
       "LR C = 0.01 weight = {0: 1, 1: 9}   0.238 (+/- 0.002)  0.239 (+/- 0.001)  \n",
       "LR C = 0.1 weight = balanced        0.381 (+/- 0.007)  0.382 (+/- 0.004)  \n",
       "LR C = 0.1 weight = {0: 1, 1: 3}    0.464 (+/- 0.018)  0.466 (+/- 0.007)  \n",
       "LR C = 0.1 weight = {0: 1, 1: 5}    0.281 (+/- 0.003)  0.282 (+/- 0.002)  \n",
       "LR C = 0.1 weight = {0: 1, 1: 7}    0.252 (+/- 0.003)  0.252 (+/- 0.001)  \n",
       "LR C = 0.1 weight = {0: 1, 1: 9}    0.239 (+/- 0.002)  0.239 (+/- 0.001)  \n",
       "LR C = 1 weight = balanced          0.381 (+/- 0.007)  0.382 (+/- 0.005)  \n",
       "LR C = 1 weight = {0: 1, 1: 3}      0.464 (+/- 0.018)  0.466 (+/- 0.007)  \n",
       "LR C = 1 weight = {0: 1, 1: 5}      0.281 (+/- 0.003)  0.282 (+/- 0.002)  \n",
       "LR C = 1 weight = {0: 1, 1: 7}      0.252 (+/- 0.003)  0.252 (+/- 0.001)  \n",
       "LR C = 1 weight = {0: 1, 1: 9}      0.239 (+/- 0.002)  0.239 (+/- 0.001)  \n",
       "LR C = 10 weight = balanced         0.380 (+/- 0.006)  0.382 (+/- 0.004)  \n",
       "LR C = 10 weight = {0: 1, 1: 3}     0.463 (+/- 0.018)  0.466 (+/- 0.007)  \n",
       "LR C = 10 weight = {0: 1, 1: 5}     0.281 (+/- 0.003)  0.282 (+/- 0.002)  \n",
       "LR C = 10 weight = {0: 1, 1: 7}     0.252 (+/- 0.003)  0.252 (+/- 0.001)  \n",
       "LR C = 10 weight = {0: 1, 1: 9}     0.239 (+/- 0.002)  0.239 (+/- 0.001)  \n",
       "LR C = 100 weight = balanced        0.381 (+/- 0.007)  0.382 (+/- 0.005)  \n",
       "LR C = 100 weight = {0: 1, 1: 3}    0.463 (+/- 0.018)  0.466 (+/- 0.007)  \n",
       "LR C = 100 weight = {0: 1, 1: 5}    0.281 (+/- 0.003)  0.282 (+/- 0.002)  \n",
       "LR C = 100 weight = {0: 1, 1: 7}    0.252 (+/- 0.003)  0.252 (+/- 0.001)  \n",
       "LR C = 100 weight = {0: 1, 1: 9}    0.239 (+/- 0.002)  0.239 (+/- 0.001)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(results).T\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since recall is the more important metric for this model, let us check from the above tables, for which values of the hyperparameters we have at least 75% recall. Since we are doing the hyperparameter optimization, we also need to check for accuracy as well. Since all the entries in the table we are looking for have a recall of greater than 0.75, we now only need to check for the best mean cross-validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from the table above, it can be concluded that the best mean validation score is for the hyperparameters of C = 1, and class_weight = {0:1, 1:5}. Since, that is the only model that has the recall of at least 75% and also have the best test/validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best C calculated is: 1\n",
      "The best class weight calculated is: {0: 1, 1: 5}\n"
     ]
    }
   ],
   "source": [
    "best_C = 1\n",
    "best_weight = {0:1, 1:5}\n",
    "\n",
    "print(\"The best C calculated is:\", best_C)\n",
    "print(\"The best class weight calculated is:\", best_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the first linear model tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first linear model tried, it can be concluded that in order to have a recall of at least 75%, we have a huge reduction in the model test accuracy to nearly 50%. Even though we have less test accuracy, there was another metric, we were more concerned of, which was recall. Since, we have set the recall to be at least 75%, which means out of all the true cases of the default, our model should predict at least 75% of them. Thus, we achieved that recall with the maximum accuracy at the hyperparameters of C = 1, and class_weight of default being 5 and non-default being 1 (0:1, 1:5). Following is the metric summary for the hyperparameters mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time           0.038 (+/- 0.003)\n",
       "score_time         0.003 (+/- 0.000)\n",
       "test_accuracy      0.509 (+/- 0.007)\n",
       "train_accuracy     0.511 (+/- 0.004)\n",
       "test_recall        0.777 (+/- 0.010)\n",
       "train_recall       0.781 (+/- 0.003)\n",
       "test_precision     0.281 (+/- 0.003)\n",
       "train_precision    0.282 (+/- 0.002)\n",
       "Name: LR C = 1 weight = {0: 1, 1: 5}, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(scores_df.loc[\"LR C = \" + str(best_C) + \" weight = \" + str(best_weight),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model to use is the Random Forest Classifier, which is a tree-based ensemble model, that creates many different decision trees and uses the majority vote to predict, whether the prediction is 1 or 0. Let's create the random forest classifier model and run a cross-validation on it, to compare the results with the linear model created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1,random_state=76, class_weight = {0:1, 1:5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.529 (+/- 0.010)</td>\n",
       "      <td>0.018 (+/- 0.001)</td>\n",
       "      <td>0.815 (+/- 0.003)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.342 (+/- 0.010)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.661 (+/- 0.018)</td>\n",
       "      <td>0.998 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 fit_time         score_time  \\\n",
       "RandomForestClassifier  0.529 (+/- 0.010)  0.018 (+/- 0.001)   \n",
       "\n",
       "                            test_accuracy     train_accuracy  \\\n",
       "RandomForestClassifier  0.815 (+/- 0.003)  1.000 (+/- 0.000)   \n",
       "\n",
       "                              test_recall       train_recall  \\\n",
       "RandomForestClassifier  0.342 (+/- 0.010)  1.000 (+/- 0.000)   \n",
       "\n",
       "                           test_precision    train_precision  \n",
       "RandomForestClassifier  0.661 (+/- 0.018)  0.998 (+/- 0.001)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_models = {}\n",
    "results_models[\"RandomForestClassifier\"] = mean_std_cross_val_scores(rf, transformed_df, y_train, cv = 5, return_train_score = True, scoring = scoring)\n",
    "scores_df = pd.DataFrame(results_models).T\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing, the results from the above table, it can be clearly seen that the model is over-fitting, as there is a lot of difference between the test_accuracy and train_accuracy. Also, we can clearly see that the model is not able to have a good or reasonable number for the test_recall, and there is a huge difference between those as well. Thus, concluding that even though we have a very high accuracy, our real and the most important metric: recall is not so huge or hasn't achieved the threshold of 75%, thus, it cannot beat the linear model (logistic regression), with the default hyperparameters. Also, the random forest classifier cannot beat the logistic regression in terms of the fit and score time, which also influences the decision of not being a good model, for this datsets, when we have default hyperparameters. Thus, There is a need for hyperparameter optimization to conclude whether it is good for this problem or not, after optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines: SVM - RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try the SVM - RBF model, and see if, we can achieve a better results or not. The SVM - RBF, is able to take some support vector features from the train datasets, which later is used to create a decision boundary. And the model uses those boundaries to predict the outcome of the examples presented. As mentioned, we are going to use the default hyperparameters for this model, but with the class_weight of {0:1, 1:5}. Since, with this class_weight we are able to compare the results of this model with the results of the linear model created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.529 (+/- 0.010)</td>\n",
       "      <td>0.018 (+/- 0.001)</td>\n",
       "      <td>0.815 (+/- 0.003)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.342 (+/- 0.010)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.661 (+/- 0.018)</td>\n",
       "      <td>0.998 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-RBF</th>\n",
       "      <td>7.975 (+/- 0.087)</td>\n",
       "      <td>2.816 (+/- 0.030)</td>\n",
       "      <td>0.691 (+/- 0.013)</td>\n",
       "      <td>0.714 (+/- 0.011)</td>\n",
       "      <td>0.694 (+/- 0.014)</td>\n",
       "      <td>0.744 (+/- 0.013)</td>\n",
       "      <td>0.390 (+/- 0.012)</td>\n",
       "      <td>0.419 (+/- 0.010)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 fit_time         score_time  \\\n",
       "RandomForestClassifier  0.529 (+/- 0.010)  0.018 (+/- 0.001)   \n",
       "SVM-RBF                 7.975 (+/- 0.087)  2.816 (+/- 0.030)   \n",
       "\n",
       "                            test_accuracy     train_accuracy  \\\n",
       "RandomForestClassifier  0.815 (+/- 0.003)  1.000 (+/- 0.000)   \n",
       "SVM-RBF                 0.691 (+/- 0.013)  0.714 (+/- 0.011)   \n",
       "\n",
       "                              test_recall       train_recall  \\\n",
       "RandomForestClassifier  0.342 (+/- 0.010)  1.000 (+/- 0.000)   \n",
       "SVM-RBF                 0.694 (+/- 0.014)  0.744 (+/- 0.013)   \n",
       "\n",
       "                           test_precision    train_precision  \n",
       "RandomForestClassifier  0.661 (+/- 0.018)  0.998 (+/- 0.001)  \n",
       "SVM-RBF                 0.390 (+/- 0.012)  0.419 (+/- 0.010)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(random_state=76, class_weight = {0:1, 1:5})\n",
    "results_models[\"SVM-RBF\"] = mean_std_cross_val_scores(svc, transformed_df, y_train, cv = 5, return_train_score = True, scoring = scoring)\n",
    "scores_df = pd.DataFrame(results_models).T\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the results for the SVM-RBF, it can clearly seen that the model is performing way better than the RandomForestClassifier, SVM-RBF has very good recall compared to the previous one, with a test recall score of 0.694 even though it is not able to achieve the threshold of 75% that was mentioned in the linear model. It has maintained to achieve an accuracy of 0.691, which is better than the linear model's (logistic regression) test accuracy. Moreover, comparing the difference of the test_accuracy and the train_accuracy, it can be said that the model may be under-fitting and it is a good indication for the hyperparameter optimization to increase the metric we want and also maintain the high accuracy at the same time. Currently, when comparing the SVM-RBF with the logistic regression model, we have created, it can be said that SVM-RBF is a good competitor for the best model, but with the default hyperparameter, it may beat the logistic regression, as we also want our accuracy to be higher, while maintaining the high recall as well. With hyperparameter optimization, we may be able to achieve a threshold of the recall, and may also beat the linear model, in terms of the accuracy as well. But, SVM-RBF has one drawback of having a high fit and score time compared to the fit and score time of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.529 (+/- 0.010)</td>\n",
       "      <td>0.018 (+/- 0.001)</td>\n",
       "      <td>0.815 (+/- 0.003)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.342 (+/- 0.010)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.661 (+/- 0.018)</td>\n",
       "      <td>0.998 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-RBF</th>\n",
       "      <td>7.975 (+/- 0.087)</td>\n",
       "      <td>2.816 (+/- 0.030)</td>\n",
       "      <td>0.691 (+/- 0.013)</td>\n",
       "      <td>0.714 (+/- 0.011)</td>\n",
       "      <td>0.694 (+/- 0.014)</td>\n",
       "      <td>0.744 (+/- 0.013)</td>\n",
       "      <td>0.390 (+/- 0.012)</td>\n",
       "      <td>0.419 (+/- 0.010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.260 (+/- 0.005)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.733 (+/- 0.008)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.393 (+/- 0.028)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.396 (+/- 0.018)</td>\n",
       "      <td>0.998 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 fit_time         score_time  \\\n",
       "RandomForestClassifier  0.529 (+/- 0.010)  0.018 (+/- 0.001)   \n",
       "SVM-RBF                 7.975 (+/- 0.087)  2.816 (+/- 0.030)   \n",
       "DecisionTree            0.260 (+/- 0.005)  0.004 (+/- 0.000)   \n",
       "\n",
       "                            test_accuracy     train_accuracy  \\\n",
       "RandomForestClassifier  0.815 (+/- 0.003)  1.000 (+/- 0.000)   \n",
       "SVM-RBF                 0.691 (+/- 0.013)  0.714 (+/- 0.011)   \n",
       "DecisionTree            0.733 (+/- 0.008)  1.000 (+/- 0.000)   \n",
       "\n",
       "                              test_recall       train_recall  \\\n",
       "RandomForestClassifier  0.342 (+/- 0.010)  1.000 (+/- 0.000)   \n",
       "SVM-RBF                 0.694 (+/- 0.014)  0.744 (+/- 0.013)   \n",
       "DecisionTree            0.393 (+/- 0.028)  1.000 (+/- 0.000)   \n",
       "\n",
       "                           test_precision    train_precision  \n",
       "RandomForestClassifier  0.661 (+/- 0.018)  0.998 (+/- 0.001)  \n",
       "SVM-RBF                 0.390 (+/- 0.012)  0.419 (+/- 0.010)  \n",
       "DecisionTree            0.396 (+/- 0.018)  0.998 (+/- 0.001)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=76, class_weight = {0:1, 1:5})\n",
    "results_models[\"DecisionTree\"] = mean_std_cross_val_scores(dt, transformed_df, y_train, cv = 5, return_train_score = True, scoring = scoring)\n",
    "scores_df = pd.DataFrame(results_models).T\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the results, it's clear to see that the DecisionTree model is overfitting; there is a large discrepancy between the validation accuracy and the training accuracy. This is also the case for the validation recall and training recall scores. Interestingly, the recall score of our single decision tree is slightly better than the score from our tree-based ensemble model. The score is still far too low however. The fitting and scoring time for the DecisionTree is the fastest out of the three models we've used here, but it is still slower than the linear model we used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization of each model tried above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.648619</td>\n",
       "      <td>0.750377</td>\n",
       "      <td>0.360297</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>0.220707</td>\n",
       "      <td>0.083013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.644048</td>\n",
       "      <td>0.756171</td>\n",
       "      <td>0.357471</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>0.667574</td>\n",
       "      <td>0.044070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.641857</td>\n",
       "      <td>0.754454</td>\n",
       "      <td>0.355614</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>0.506337</td>\n",
       "      <td>0.049976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.640714</td>\n",
       "      <td>0.759390</td>\n",
       "      <td>0.355399</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>0.745809</td>\n",
       "      <td>0.050190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.639667</td>\n",
       "      <td>0.755314</td>\n",
       "      <td>0.354103</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.363849</td>\n",
       "      <td>0.043858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.639667</td>\n",
       "      <td>0.759391</td>\n",
       "      <td>0.354897</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.083376</td>\n",
       "      <td>0.033906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.638857</td>\n",
       "      <td>0.761750</td>\n",
       "      <td>0.354316</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>0.906017</td>\n",
       "      <td>0.043428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.636286</td>\n",
       "      <td>0.764541</td>\n",
       "      <td>0.352855</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>0.928192</td>\n",
       "      <td>0.061893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.633048</td>\n",
       "      <td>0.766903</td>\n",
       "      <td>0.350868</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0.137405</td>\n",
       "      <td>0.063192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.511048</td>\n",
       "      <td>0.846111</td>\n",
       "      <td>0.294746</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.112644</td>\n",
       "      <td>0.033818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.444143</td>\n",
       "      <td>0.896539</td>\n",
       "      <td>0.273791</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.091285</td>\n",
       "      <td>0.043689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.382571</td>\n",
       "      <td>0.931529</td>\n",
       "      <td>0.255771</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.124847</td>\n",
       "      <td>0.034813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.364143</td>\n",
       "      <td>0.853863</td>\n",
       "      <td>0.242533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025205</td>\n",
       "      <td>0.008817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.329143</td>\n",
       "      <td>0.955572</td>\n",
       "      <td>0.242867</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.221040</td>\n",
       "      <td>0.056187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.328048</td>\n",
       "      <td>0.955572</td>\n",
       "      <td>0.242578</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0.298434</td>\n",
       "      <td>0.073126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.301048</td>\n",
       "      <td>0.965878</td>\n",
       "      <td>0.236722</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410838</td>\n",
       "      <td>0.054248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.294476</td>\n",
       "      <td>0.966093</td>\n",
       "      <td>0.235303</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0.204597</td>\n",
       "      <td>0.042037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.294429</td>\n",
       "      <td>0.968026</td>\n",
       "      <td>0.235443</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>0.480390</td>\n",
       "      <td>0.045690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.972103</td>\n",
       "      <td>0.232560</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>0.358788</td>\n",
       "      <td>0.073641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean_test_accuracy  mean_test_recall  mean_test_precision  \\\n",
       "rank_test_accuracy                                                              \n",
       "77                            0.648619          0.750377             0.360297   \n",
       "79                            0.644048          0.756171             0.357471   \n",
       "80                            0.641857          0.754454             0.355614   \n",
       "81                            0.640714          0.759390             0.355399   \n",
       "82                            0.639667          0.755314             0.354103   \n",
       "83                            0.639667          0.759391             0.354897   \n",
       "84                            0.638857          0.761750             0.354316   \n",
       "86                            0.636286          0.764541             0.352855   \n",
       "87                            0.633048          0.766903             0.350868   \n",
       "91                            0.511048          0.846111             0.294746   \n",
       "92                            0.444143          0.896539             0.273791   \n",
       "93                            0.382571          0.931529             0.255771   \n",
       "94                            0.364143          0.853863             0.242533   \n",
       "95                            0.329143          0.955572             0.242867   \n",
       "96                            0.328048          0.955572             0.242578   \n",
       "97                            0.301048          0.965878             0.236722   \n",
       "98                            0.294476          0.966093             0.235303   \n",
       "99                            0.294429          0.968026             0.235443   \n",
       "100                           0.281000          0.972103             0.232560   \n",
       "\n",
       "                   param_n_estimators param_max_depth  mean_fit_time  \\\n",
       "rank_test_accuracy                                                     \n",
       "77                                 31               3       0.220707   \n",
       "79                                 61               3       0.667574   \n",
       "80                                 51               3       0.506337   \n",
       "81                                 71               3       0.745809   \n",
       "82                                 41               3       0.363849   \n",
       "83                                 11               3       0.083376   \n",
       "84                                 81               3       0.906017   \n",
       "86                                 91               3       0.928192   \n",
       "87                                 21               3       0.137405   \n",
       "91                                 21               1       0.112644   \n",
       "92                                 11               1       0.091285   \n",
       "93                                 31               1       0.124847   \n",
       "94                                  1               1       0.025205   \n",
       "95                                 51               1       0.221040   \n",
       "96                                 61               1       0.298434   \n",
       "97                                 81               1       0.410838   \n",
       "98                                 41               1       0.204597   \n",
       "99                                 91               1       0.480390   \n",
       "100                                71               1       0.358788   \n",
       "\n",
       "                    mean_score_time  \n",
       "rank_test_accuracy                   \n",
       "77                         0.083013  \n",
       "79                         0.044070  \n",
       "80                         0.049976  \n",
       "81                         0.050190  \n",
       "82                         0.043858  \n",
       "83                         0.033906  \n",
       "84                         0.043428  \n",
       "86                         0.061893  \n",
       "87                         0.063192  \n",
       "91                         0.033818  \n",
       "92                         0.043689  \n",
       "93                         0.034813  \n",
       "94                         0.008817  \n",
       "95                         0.056187  \n",
       "96                         0.073126  \n",
       "97                         0.054248  \n",
       "98                         0.042037  \n",
       "99                         0.045690  \n",
       "100                        0.073641  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_param_grid = {\"n_estimators\": np.arange(1, 100, 10), \n",
    "              \"max_depth\": np.arange(1, 20, 2)}\n",
    "rf_random_search = RandomizedSearchCV(rf, param_distributions=rf_param_grid, n_iter=100, n_jobs=-1, return_train_score=True, scoring=scoring, refit='accuracy', random_state=76)\n",
    "rf_random_search.fit(transformed_df, y_train)\n",
    "rf_optimization_df = pd.DataFrame(rf_random_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_accuracy\",\n",
    "        \"mean_test_recall\",\n",
    "        \"mean_test_precision\",\n",
    "        \"param_n_estimators\",\n",
    "        \"param_max_depth\",\n",
    "        \"mean_fit_time\",\n",
    "        \"mean_score_time\",\n",
    "        \"rank_test_accuracy\",\n",
    "    ]\n",
    "].set_index(\"rank_test_accuracy\").sort_index()\n",
    "rf_optimization_df[rf_optimization_df[\"mean_test_recall\"] > 0.75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe above ranks the accuracy for the given hyperparameters that have a recall score greater than 75. As we can see, the largest accuracy score we can get is 0.65 with n_estimators = 31 and max_depth = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.777381</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.401052</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>54.685680</td>\n",
       "      <td>5.148599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.777381</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.401052</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>55.561646</td>\n",
       "      <td>5.729901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.777381</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.401052</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>55.061012</td>\n",
       "      <td>5.851923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.767714</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>0.344566</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>57.867143</td>\n",
       "      <td>5.944829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.767190</td>\n",
       "      <td>0.052374</td>\n",
       "      <td>0.339034</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>59.549481</td>\n",
       "      <td>6.248690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.761952</td>\n",
       "      <td>0.059887</td>\n",
       "      <td>0.309232</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>56.323935</td>\n",
       "      <td>6.369174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.740857</td>\n",
       "      <td>0.262936</td>\n",
       "      <td>0.378869</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>45.990910</td>\n",
       "      <td>4.494696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.735143</td>\n",
       "      <td>0.280108</td>\n",
       "      <td>0.371417</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>40.571582</td>\n",
       "      <td>4.887601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.727524</td>\n",
       "      <td>0.336772</td>\n",
       "      <td>0.373411</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37.260284</td>\n",
       "      <td>5.569106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.726333</td>\n",
       "      <td>0.648640</td>\n",
       "      <td>0.424275</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16.932189</td>\n",
       "      <td>4.835847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.702905</td>\n",
       "      <td>0.675259</td>\n",
       "      <td>0.400531</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>14.360280</td>\n",
       "      <td>4.907055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.693619</td>\n",
       "      <td>0.689849</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>15.836441</td>\n",
       "      <td>4.897028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.691476</td>\n",
       "      <td>0.607640</td>\n",
       "      <td>0.378491</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>27.644807</td>\n",
       "      <td>4.162403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.684762</td>\n",
       "      <td>0.523720</td>\n",
       "      <td>0.356633</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>62.862036</td>\n",
       "      <td>3.668443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.683143</td>\n",
       "      <td>0.693068</td>\n",
       "      <td>0.382261</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>28.805404</td>\n",
       "      <td>4.543028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.682905</td>\n",
       "      <td>0.690923</td>\n",
       "      <td>0.381707</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15.737138</td>\n",
       "      <td>4.590297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.673286</td>\n",
       "      <td>0.688347</td>\n",
       "      <td>0.373034</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15.887285</td>\n",
       "      <td>5.311685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.672952</td>\n",
       "      <td>0.688563</td>\n",
       "      <td>0.372409</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16.156333</td>\n",
       "      <td>5.257931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.665905</td>\n",
       "      <td>0.711315</td>\n",
       "      <td>0.369168</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>16.468470</td>\n",
       "      <td>5.049345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.650286</td>\n",
       "      <td>0.695646</td>\n",
       "      <td>0.354172</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>15.367821</td>\n",
       "      <td>5.289705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.606000</td>\n",
       "      <td>0.740935</td>\n",
       "      <td>0.328245</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16.356795</td>\n",
       "      <td>5.532462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.591238</td>\n",
       "      <td>0.755315</td>\n",
       "      <td>0.321103</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>16.396555</td>\n",
       "      <td>5.370718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.544619</td>\n",
       "      <td>0.774850</td>\n",
       "      <td>0.297864</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>17.098979</td>\n",
       "      <td>5.882953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.386714</td>\n",
       "      <td>0.908135</td>\n",
       "      <td>0.253631</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.803275</td>\n",
       "      <td>6.236699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.224762</td>\n",
       "      <td>0.998498</td>\n",
       "      <td>0.222318</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>26.336139</td>\n",
       "      <td>6.413938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.222143</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>0.221841</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>20.051439</td>\n",
       "      <td>6.126739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.221952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221878</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>17.501545</td>\n",
       "      <td>5.955634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>19.960928</td>\n",
       "      <td>6.010618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>25.178093</td>\n",
       "      <td>6.461759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>19.493587</td>\n",
       "      <td>5.451848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>24.332035</td>\n",
       "      <td>6.308110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>21.636338</td>\n",
       "      <td>6.452984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20.169509</td>\n",
       "      <td>5.894542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20.405425</td>\n",
       "      <td>5.931984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>21.844202</td>\n",
       "      <td>5.409145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.221857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>20.147785</td>\n",
       "      <td>5.567954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean_test_accuracy  mean_test_recall  mean_test_precision  \\\n",
       "rank_test_accuracy                                                              \n",
       "1                             0.777381          0.006655             0.401052   \n",
       "1                             0.777381          0.006655             0.401052   \n",
       "1                             0.777381          0.006655             0.401052   \n",
       "4                             0.767714          0.052158             0.344566   \n",
       "5                             0.767190          0.052374             0.339034   \n",
       "6                             0.761952          0.059887             0.309232   \n",
       "7                             0.740857          0.262936             0.378869   \n",
       "8                             0.735143          0.280108             0.371417   \n",
       "9                             0.727524          0.336772             0.373411   \n",
       "10                            0.726333          0.648640             0.424275   \n",
       "11                            0.702905          0.675259             0.400531   \n",
       "12                            0.693619          0.689849             0.392344   \n",
       "13                            0.691476          0.607640             0.378491   \n",
       "14                            0.684762          0.523720             0.356633   \n",
       "15                            0.683143          0.693068             0.382261   \n",
       "16                            0.682905          0.690923             0.381707   \n",
       "17                            0.673286          0.688347             0.373034   \n",
       "18                            0.672952          0.688563             0.372409   \n",
       "19                            0.665905          0.711315             0.369168   \n",
       "20                            0.650286          0.695646             0.354172   \n",
       "21                            0.606000          0.740935             0.328245   \n",
       "22                            0.591238          0.755315             0.321103   \n",
       "23                            0.544619          0.774850             0.297864   \n",
       "24                            0.386714          0.908135             0.253631   \n",
       "25                            0.224762          0.998498             0.222318   \n",
       "26                            0.222143          0.999356             0.221841   \n",
       "27                            0.221952          1.000000             0.221878   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "28                            0.221857          1.000000             0.221857   \n",
       "\n",
       "                   param_C param_gamma  mean_fit_time  mean_score_time  \n",
       "rank_test_accuracy                                                      \n",
       "1                      100         100      54.685680         5.148599  \n",
       "1                        1         100      55.561646         5.729901  \n",
       "1                       10         100      55.061012         5.851923  \n",
       "4                      100          10      57.867143         5.944829  \n",
       "5                       10          10      59.549481         6.248690  \n",
       "6                        1          10      56.323935         6.369174  \n",
       "7                      100           1      45.990910         4.494696  \n",
       "8                       10           1      40.571582         4.887601  \n",
       "9                        1           1      37.260284         5.569106  \n",
       "10                     100       0.001      16.932189         4.835847  \n",
       "11                       1        0.01      14.360280         4.907055  \n",
       "12                      10        0.01      15.836441         4.897028  \n",
       "13                      10         0.1      27.644807         4.162403  \n",
       "14                     100         0.1      62.862036         3.668443  \n",
       "15                     100        0.01      28.805404         4.543028  \n",
       "16                       1         0.1      15.737138         4.590297  \n",
       "17                      10       0.001      15.887285         5.311685  \n",
       "18                     0.1        0.01      16.156333         5.257931  \n",
       "19                     0.1         0.1      16.468470         5.049345  \n",
       "20                       1       0.001      15.367821         5.289705  \n",
       "21                    0.01        0.01      16.356795         5.532462  \n",
       "22                    0.01         0.1      16.396555         5.370718  \n",
       "23                     0.1       0.001      17.098979         5.882953  \n",
       "24                     0.1           1      20.803275         6.236699  \n",
       "25                     0.1          10      26.336139         6.413938  \n",
       "26                    0.01           1      20.051439         6.126739  \n",
       "27                    0.01       0.001      17.501545         5.955634  \n",
       "28                   0.001       0.001      19.960928         6.010618  \n",
       "28                    0.01          10      25.178093         6.461759  \n",
       "28                   0.001         100      19.493587         5.451848  \n",
       "28                   0.001          10      24.332035         6.308110  \n",
       "28                   0.001           1      21.636338         6.452984  \n",
       "28                   0.001         0.1      20.169509         5.894542  \n",
       "28                   0.001        0.01      20.405425         5.931984  \n",
       "28                    0.01         100      21.844202         5.409145  \n",
       "28                     0.1         100      20.147785         5.567954  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_param_grid = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100], \"gamma\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "svc_random_search = RandomizedSearchCV(svc, param_distributions=svc_param_grid, n_iter=36, n_jobs=-1, return_train_score=True, scoring=scoring, refit='accuracy', random_state=76)\n",
    "svc_random_search.fit(transformed_df, y_train)\n",
    "svc_optimization_df = pd.DataFrame(svc_random_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_accuracy\",\n",
    "        \"mean_test_recall\",\n",
    "        \"mean_test_precision\",\n",
    "        \"param_C\",\n",
    "        \"param_gamma\",\n",
    "        \"mean_fit_time\",\n",
    "        \"mean_score_time\",\n",
    "        \"rank_test_accuracy\",\n",
    "    ]\n",
    "].set_index(\"rank_test_accuracy\").sort_index()\n",
    "\n",
    "svc_optimization_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataframe above, we can see that C = 0.01 and gamma = 0.1 will give us the highest accuracy of 0.59 with a recall score greater than 75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
      "/var/folders/z3/__52hny91dz_m7bg8pd9fvg80000gn/T/ipykernel_1863/3026881700.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>depth = 1</th>\n",
       "      <td>0.018 (+/- 0.001)</td>\n",
       "      <td>0.004 (+/- 0.001)</td>\n",
       "      <td>0.819 (+/- 0.005)</td>\n",
       "      <td>0.819 (+/- 0.001)</td>\n",
       "      <td>0.327 (+/- 0.010)</td>\n",
       "      <td>0.327 (+/- 0.003)</td>\n",
       "      <td>0.695 (+/- 0.027)</td>\n",
       "      <td>0.694 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 2</th>\n",
       "      <td>0.034 (+/- 0.000)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.819 (+/- 0.005)</td>\n",
       "      <td>0.819 (+/- 0.001)</td>\n",
       "      <td>0.322 (+/- 0.013)</td>\n",
       "      <td>0.322 (+/- 0.007)</td>\n",
       "      <td>0.699 (+/- 0.027)</td>\n",
       "      <td>0.702 (+/- 0.010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 3</th>\n",
       "      <td>0.050 (+/- 0.000)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.818 (+/- 0.005)</td>\n",
       "      <td>0.821 (+/- 0.001)</td>\n",
       "      <td>0.369 (+/- 0.020)</td>\n",
       "      <td>0.373 (+/- 0.032)</td>\n",
       "      <td>0.665 (+/- 0.040)</td>\n",
       "      <td>0.675 (+/- 0.020)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 4</th>\n",
       "      <td>0.067 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.819 (+/- 0.004)</td>\n",
       "      <td>0.823 (+/- 0.001)</td>\n",
       "      <td>0.371 (+/- 0.019)</td>\n",
       "      <td>0.377 (+/- 0.008)</td>\n",
       "      <td>0.665 (+/- 0.020)</td>\n",
       "      <td>0.684 (+/- 0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 5</th>\n",
       "      <td>0.083 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.818 (+/- 0.003)</td>\n",
       "      <td>0.825 (+/- 0.001)</td>\n",
       "      <td>0.370 (+/- 0.024)</td>\n",
       "      <td>0.380 (+/- 0.013)</td>\n",
       "      <td>0.660 (+/- 0.019)</td>\n",
       "      <td>0.690 (+/- 0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 6</th>\n",
       "      <td>0.097 (+/- 0.000)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.817 (+/- 0.003)</td>\n",
       "      <td>0.828 (+/- 0.001)</td>\n",
       "      <td>0.381 (+/- 0.020)</td>\n",
       "      <td>0.402 (+/- 0.018)</td>\n",
       "      <td>0.649 (+/- 0.020)</td>\n",
       "      <td>0.694 (+/- 0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 7</th>\n",
       "      <td>0.112 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.814 (+/- 0.004)</td>\n",
       "      <td>0.832 (+/- 0.001)</td>\n",
       "      <td>0.366 (+/- 0.028)</td>\n",
       "      <td>0.401 (+/- 0.021)</td>\n",
       "      <td>0.644 (+/- 0.023)</td>\n",
       "      <td>0.717 (+/- 0.023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 8</th>\n",
       "      <td>0.127 (+/- 0.000)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.811 (+/- 0.004)</td>\n",
       "      <td>0.838 (+/- 0.001)</td>\n",
       "      <td>0.367 (+/- 0.029)</td>\n",
       "      <td>0.421 (+/- 0.024)</td>\n",
       "      <td>0.624 (+/- 0.016)</td>\n",
       "      <td>0.737 (+/- 0.023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 9</th>\n",
       "      <td>0.142 (+/- 0.001)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.808 (+/- 0.004)</td>\n",
       "      <td>0.847 (+/- 0.002)</td>\n",
       "      <td>0.369 (+/- 0.018)</td>\n",
       "      <td>0.450 (+/- 0.009)</td>\n",
       "      <td>0.614 (+/- 0.015)</td>\n",
       "      <td>0.761 (+/- 0.014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 10</th>\n",
       "      <td>0.156 (+/- 0.000)</td>\n",
       "      <td>0.003 (+/- 0.000)</td>\n",
       "      <td>0.802 (+/- 0.003)</td>\n",
       "      <td>0.856 (+/- 0.002)</td>\n",
       "      <td>0.371 (+/- 0.022)</td>\n",
       "      <td>0.486 (+/- 0.018)</td>\n",
       "      <td>0.585 (+/- 0.014)</td>\n",
       "      <td>0.784 (+/- 0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 11</th>\n",
       "      <td>0.173 (+/- 0.007)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.800 (+/- 0.003)</td>\n",
       "      <td>0.865 (+/- 0.002)</td>\n",
       "      <td>0.377 (+/- 0.023)</td>\n",
       "      <td>0.518 (+/- 0.018)</td>\n",
       "      <td>0.575 (+/- 0.010)</td>\n",
       "      <td>0.806 (+/- 0.014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 12</th>\n",
       "      <td>0.183 (+/- 0.001)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.791 (+/- 0.001)</td>\n",
       "      <td>0.877 (+/- 0.003)</td>\n",
       "      <td>0.380 (+/- 0.034)</td>\n",
       "      <td>0.561 (+/- 0.027)</td>\n",
       "      <td>0.542 (+/- 0.007)</td>\n",
       "      <td>0.833 (+/- 0.029)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 13</th>\n",
       "      <td>0.200 (+/- 0.001)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.789 (+/- 0.003)</td>\n",
       "      <td>0.888 (+/- 0.005)</td>\n",
       "      <td>0.377 (+/- 0.019)</td>\n",
       "      <td>0.593 (+/- 0.017)</td>\n",
       "      <td>0.535 (+/- 0.010)</td>\n",
       "      <td>0.862 (+/- 0.016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 14</th>\n",
       "      <td>0.208 (+/- 0.002)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.783 (+/- 0.003)</td>\n",
       "      <td>0.900 (+/- 0.004)</td>\n",
       "      <td>0.376 (+/- 0.019)</td>\n",
       "      <td>0.630 (+/- 0.020)</td>\n",
       "      <td>0.516 (+/- 0.010)</td>\n",
       "      <td>0.888 (+/- 0.020)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 15</th>\n",
       "      <td>0.221 (+/- 0.005)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.780 (+/- 0.003)</td>\n",
       "      <td>0.911 (+/- 0.004)</td>\n",
       "      <td>0.380 (+/- 0.015)</td>\n",
       "      <td>0.664 (+/- 0.011)</td>\n",
       "      <td>0.505 (+/- 0.011)</td>\n",
       "      <td>0.912 (+/- 0.022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 16</th>\n",
       "      <td>0.233 (+/- 0.003)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.771 (+/- 0.003)</td>\n",
       "      <td>0.921 (+/- 0.004)</td>\n",
       "      <td>0.385 (+/- 0.007)</td>\n",
       "      <td>0.705 (+/- 0.009)</td>\n",
       "      <td>0.480 (+/- 0.009)</td>\n",
       "      <td>0.921 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 17</th>\n",
       "      <td>0.242 (+/- 0.004)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.764 (+/- 0.005)</td>\n",
       "      <td>0.931 (+/- 0.004)</td>\n",
       "      <td>0.379 (+/- 0.012)</td>\n",
       "      <td>0.736 (+/- 0.007)</td>\n",
       "      <td>0.461 (+/- 0.014)</td>\n",
       "      <td>0.940 (+/- 0.015)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 18</th>\n",
       "      <td>0.247 (+/- 0.003)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.758 (+/- 0.007)</td>\n",
       "      <td>0.942 (+/- 0.005)</td>\n",
       "      <td>0.388 (+/- 0.017)</td>\n",
       "      <td>0.780 (+/- 0.015)</td>\n",
       "      <td>0.449 (+/- 0.017)</td>\n",
       "      <td>0.948 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth = 19</th>\n",
       "      <td>0.259 (+/- 0.003)</td>\n",
       "      <td>0.004 (+/- 0.000)</td>\n",
       "      <td>0.754 (+/- 0.008)</td>\n",
       "      <td>0.951 (+/- 0.006)</td>\n",
       "      <td>0.384 (+/- 0.009)</td>\n",
       "      <td>0.811 (+/- 0.015)</td>\n",
       "      <td>0.438 (+/- 0.018)</td>\n",
       "      <td>0.964 (+/- 0.016)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     fit_time         score_time      test_accuracy  \\\n",
       "depth = 1   0.018 (+/- 0.001)  0.004 (+/- 0.001)  0.819 (+/- 0.005)   \n",
       "depth = 2   0.034 (+/- 0.000)  0.003 (+/- 0.000)  0.819 (+/- 0.005)   \n",
       "depth = 3   0.050 (+/- 0.000)  0.003 (+/- 0.000)  0.818 (+/- 0.005)   \n",
       "depth = 4   0.067 (+/- 0.001)  0.003 (+/- 0.000)  0.819 (+/- 0.004)   \n",
       "depth = 5   0.083 (+/- 0.001)  0.003 (+/- 0.000)  0.818 (+/- 0.003)   \n",
       "depth = 6   0.097 (+/- 0.000)  0.003 (+/- 0.000)  0.817 (+/- 0.003)   \n",
       "depth = 7   0.112 (+/- 0.001)  0.003 (+/- 0.000)  0.814 (+/- 0.004)   \n",
       "depth = 8   0.127 (+/- 0.000)  0.003 (+/- 0.000)  0.811 (+/- 0.004)   \n",
       "depth = 9   0.142 (+/- 0.001)  0.003 (+/- 0.000)  0.808 (+/- 0.004)   \n",
       "depth = 10  0.156 (+/- 0.000)  0.003 (+/- 0.000)  0.802 (+/- 0.003)   \n",
       "depth = 11  0.173 (+/- 0.007)  0.004 (+/- 0.000)  0.800 (+/- 0.003)   \n",
       "depth = 12  0.183 (+/- 0.001)  0.004 (+/- 0.000)  0.791 (+/- 0.001)   \n",
       "depth = 13  0.200 (+/- 0.001)  0.004 (+/- 0.000)  0.789 (+/- 0.003)   \n",
       "depth = 14  0.208 (+/- 0.002)  0.004 (+/- 0.000)  0.783 (+/- 0.003)   \n",
       "depth = 15  0.221 (+/- 0.005)  0.004 (+/- 0.000)  0.780 (+/- 0.003)   \n",
       "depth = 16  0.233 (+/- 0.003)  0.004 (+/- 0.000)  0.771 (+/- 0.003)   \n",
       "depth = 17  0.242 (+/- 0.004)  0.004 (+/- 0.000)  0.764 (+/- 0.005)   \n",
       "depth = 18  0.247 (+/- 0.003)  0.004 (+/- 0.000)  0.758 (+/- 0.007)   \n",
       "depth = 19  0.259 (+/- 0.003)  0.004 (+/- 0.000)  0.754 (+/- 0.008)   \n",
       "\n",
       "               train_accuracy        test_recall       train_recall  \\\n",
       "depth = 1   0.819 (+/- 0.001)  0.327 (+/- 0.010)  0.327 (+/- 0.003)   \n",
       "depth = 2   0.819 (+/- 0.001)  0.322 (+/- 0.013)  0.322 (+/- 0.007)   \n",
       "depth = 3   0.821 (+/- 0.001)  0.369 (+/- 0.020)  0.373 (+/- 0.032)   \n",
       "depth = 4   0.823 (+/- 0.001)  0.371 (+/- 0.019)  0.377 (+/- 0.008)   \n",
       "depth = 5   0.825 (+/- 0.001)  0.370 (+/- 0.024)  0.380 (+/- 0.013)   \n",
       "depth = 6   0.828 (+/- 0.001)  0.381 (+/- 0.020)  0.402 (+/- 0.018)   \n",
       "depth = 7   0.832 (+/- 0.001)  0.366 (+/- 0.028)  0.401 (+/- 0.021)   \n",
       "depth = 8   0.838 (+/- 0.001)  0.367 (+/- 0.029)  0.421 (+/- 0.024)   \n",
       "depth = 9   0.847 (+/- 0.002)  0.369 (+/- 0.018)  0.450 (+/- 0.009)   \n",
       "depth = 10  0.856 (+/- 0.002)  0.371 (+/- 0.022)  0.486 (+/- 0.018)   \n",
       "depth = 11  0.865 (+/- 0.002)  0.377 (+/- 0.023)  0.518 (+/- 0.018)   \n",
       "depth = 12  0.877 (+/- 0.003)  0.380 (+/- 0.034)  0.561 (+/- 0.027)   \n",
       "depth = 13  0.888 (+/- 0.005)  0.377 (+/- 0.019)  0.593 (+/- 0.017)   \n",
       "depth = 14  0.900 (+/- 0.004)  0.376 (+/- 0.019)  0.630 (+/- 0.020)   \n",
       "depth = 15  0.911 (+/- 0.004)  0.380 (+/- 0.015)  0.664 (+/- 0.011)   \n",
       "depth = 16  0.921 (+/- 0.004)  0.385 (+/- 0.007)  0.705 (+/- 0.009)   \n",
       "depth = 17  0.931 (+/- 0.004)  0.379 (+/- 0.012)  0.736 (+/- 0.007)   \n",
       "depth = 18  0.942 (+/- 0.005)  0.388 (+/- 0.017)  0.780 (+/- 0.015)   \n",
       "depth = 19  0.951 (+/- 0.006)  0.384 (+/- 0.009)  0.811 (+/- 0.015)   \n",
       "\n",
       "               test_precision    train_precision  \n",
       "depth = 1   0.695 (+/- 0.027)  0.694 (+/- 0.007)  \n",
       "depth = 2   0.699 (+/- 0.027)  0.702 (+/- 0.010)  \n",
       "depth = 3   0.665 (+/- 0.040)  0.675 (+/- 0.020)  \n",
       "depth = 4   0.665 (+/- 0.020)  0.684 (+/- 0.005)  \n",
       "depth = 5   0.660 (+/- 0.019)  0.690 (+/- 0.013)  \n",
       "depth = 6   0.649 (+/- 0.020)  0.694 (+/- 0.013)  \n",
       "depth = 7   0.644 (+/- 0.023)  0.717 (+/- 0.023)  \n",
       "depth = 8   0.624 (+/- 0.016)  0.737 (+/- 0.023)  \n",
       "depth = 9   0.614 (+/- 0.015)  0.761 (+/- 0.014)  \n",
       "depth = 10  0.585 (+/- 0.014)  0.784 (+/- 0.013)  \n",
       "depth = 11  0.575 (+/- 0.010)  0.806 (+/- 0.014)  \n",
       "depth = 12  0.542 (+/- 0.007)  0.833 (+/- 0.029)  \n",
       "depth = 13  0.535 (+/- 0.010)  0.862 (+/- 0.016)  \n",
       "depth = 14  0.516 (+/- 0.010)  0.888 (+/- 0.020)  \n",
       "depth = 15  0.505 (+/- 0.011)  0.912 (+/- 0.022)  \n",
       "depth = 16  0.480 (+/- 0.009)  0.921 (+/- 0.018)  \n",
       "depth = 17  0.461 (+/- 0.014)  0.940 (+/- 0.015)  \n",
       "depth = 18  0.449 (+/- 0.017)  0.948 (+/- 0.018)  \n",
       "depth = 19  0.438 (+/- 0.018)  0.964 (+/- 0.016)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_grid = {\"max_depth\": np.arange(1, 20)}\n",
    "result_dict = {\"depth\":[], \"train_score\":[], \"test_score\":[], \"train_recall\":[], \"test_recall\":[], \"train_precision\":[], \"test_precision\":[]}\n",
    "results = {}\n",
    "\n",
    "for depth in param_grid[\"max_depth\"]:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth)\n",
    "    scores = mean_std_cross_val_scores(dt, transformed_df, y_train, cv = 5, return_train_score = True, scoring = scoring)\n",
    "    results[\"depth = \" + str(depth)] = scores\n",
    "scores_df = pd.DataFrame(results).T\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that for a decision tree, the recall score stays at around the same value no matter the depth of the tree. It never gets to a recall score > 75 and is not better than the linear model for what we're looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cpsc330/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data: 68.49%\n",
      "Recall on Test Data: 71.52%\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "\n",
    "best_c = 1\n",
    "best_gamma = 0.1\n",
    "class_weight = {0: 1, 1: 5}\n",
    "\n",
    "best_svm_model = SVC(kernel='rbf', C=best_c, gamma=best_gamma, class_weight=class_weight, random_state=76)\n",
    "best_svm_model.fit(transformed_df, y_train)\n",
    "\n",
    "y_pred_test = best_svm_model.predict(X_test_scaled)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy on Test Data: {accuracy_test * 100:.2f}%\")\n",
    "print(f\"Recall on Test Data: {recall_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The best model identified is the SVM with RBF kernel. The model was trained on the training data and evaluated on the test data with the following results:\n",
    "Accuracy: 68.49%, Recall: 71.52%\n",
    "The test accuracy of 68.49% is roughly the same as the validation accuracy mentioned previously (around 71% for the initial SVM-RBF without hyperparameter tuning). The recall for the test data is 71.52%, indicating that the model is reasonably good with identifying positive cases (default payments).\n",
    "\n",
    "2. The results indicate that the SVM-RBF model performs reasonably well on the majority class (non-default) but still has room for slight improvement in identifying the minority class (default). Regarding optimization bias, there is no potential concern due to the significant difference between accuracy and recall for the minority class. The model's performance indicates that it may not be biased towards the majority class (non-defaults). Thus, the results are promising and the model is likely not overfitting. However, the recall for the minority class may still benefit from further adjustments and tuning to improve minority class detection.\n",
    "\n",
    "To address these issues, we could use:\n",
    "- Techniques such as SMOTE (Synthetic Minority Over-sampling Technique) or adjusting class weights in the SVM could be employed.\n",
    "- More extensive hyperparameter tuning to help in finding an even better balance between precision and recall.\n",
    "- Combine multiple models to help in improving the recall while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of important results\n",
    "\n",
    "| Metric | Training Data | Testing Data\n",
    "| ----- | ------------- | --------- |\n",
    "| Accuracy | 71% | 68.49% |\n",
    "| Recall (Minority Class) | ~74% | 71.52% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The SVM-RBF model shows promise with a high accuracy. It can still do better handling of the minority class (default payments). After implementing class balancing techniques and hyperparameter tuning, the model's performance improved.\n",
    "\n",
    "Potential Improvements we did not try but could potentially improve the performance:\n",
    "\n",
    "- Class Imbalance Handling: Techniques such as SMOTE (Synthetic Minority Over-sampling Technique) or adjusting class weights in the SVM could be employed to better handle the class imbalance.\n",
    "- Ensemble Methods: Combining multiple models might help in improving the recall while maintaining high accuracy.\n",
    "- Feature Engineering: Creating new features or transforming existing ones to capture more information could improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
